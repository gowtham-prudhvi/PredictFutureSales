{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nfrom itertools import product\n\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n","execution_count":124,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n","execution_count":125,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"item_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nsales_train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\nsample_submission = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","execution_count":126,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(sales_train.head())\nprint(test.head())\nprint(items.head())","execution_count":127,"outputs":[{"output_type":"stream","text":"         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n0  02.01.2013               0       59    22154      999.00           1.0\n1  03.01.2013               0       25     2552      899.00           1.0\n2  05.01.2013               0       25     2552      899.00          -1.0\n3  06.01.2013               0       25     2554     1709.05           1.0\n4  15.01.2013               0       25     2555     1099.00           1.0\n   ID  shop_id  item_id\n0   0        5     5037\n1   1        5     5320\n2   2        5     5233\n3   3        5     5232\n4   4        5     5268\n                                           item_name  item_id  \\\n0          ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0   \n1  !ABBYY FineReader 12 Professional Edition Full...        1   \n2      ***В ЛУЧАХ СЛАВЫ   (UNV)                    D        2   \n3    ***ГОЛУБАЯ ВОЛНА  (Univ)                      D        3   \n4        ***КОРОБКА (СТЕКЛО)                       D        4   \n\n   item_category_id  \n0                40  \n1                76  \n2                40  \n3                40  \n4                40  \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_temp = test.copy()\ntest_temp['date_block_num'] = 34\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\ntest_temp = pd.merge(test_temp, item_category_mapping, how='left', on='item_id')\ntest_temp = test_temp.drop('ID',axis = 1)\nsales_train = sales_train.drop('date', axis = 1)","execution_count":128,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"sales_train_full = pd.concat([sales_train,test_temp], ignore_index = True)","execution_count":129,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales_train_full\n\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\n#all_data2 = all_data.copy()\ndel grid, gb \ngc.collect();","execution_count":131,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/generic.py:1455: FutureWarning: using a dict with renaming is deprecated and will be removed\nin a future version.\n\nFor column-specific groupby renaming, use named aggregation\n\n    >>> df.groupby(...).agg(name=('column', aggfunc))\n\n  return super().aggregate(arg, *args, **kwargs)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\nall_data = downcast_dtypes(all_data)\ngc.collect();","execution_count":132,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  \n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=6), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb10e1c4b184948a0c5742223089fa2"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data","execution_count":133,"outputs":[{"output_type":"execute_result","execution_count":133,"data":{"text/plain":"         shop_id  item_id  date_block_num  target  target_shop  target_item  \\\n0             54    10297              12     4.0       8198.0         23.0   \n1             54    10296              12     3.0       8198.0         17.0   \n2             54    10298              12    14.0       8198.0        182.0   \n3             54    10300              12     3.0       8198.0         26.0   \n4             54    10284              12     1.0       8198.0          3.0   \n...          ...      ...             ...     ...          ...          ...   \n6639289       45    18454              34     0.0          0.0          0.0   \n6639290       45    16188              34     0.0          0.0          0.0   \n6639291       45    15757              34     0.0          0.0          0.0   \n6639292       45    19648              34     0.0          0.0          0.0   \n6639293       45      969              34     0.0          0.0          0.0   \n\n         target_lag_1  target_item_lag_1  target_shop_lag_1  target_lag_2  \\\n0                 3.0               42.0            10055.0           0.0   \n1                 0.0               24.0            10055.0           0.0   \n2                21.0              369.0            10055.0         119.0   \n3                 1.0               54.0            10055.0          31.0   \n4                 0.0                4.0            10055.0           0.0   \n...               ...                ...                ...           ...   \n6639289           1.0                2.0              702.0           0.0   \n6639290           0.0                1.0              702.0           0.0   \n6639291           0.0                5.0              702.0           0.0   \n6639292           0.0                2.0              702.0           0.0   \n6639293           0.0                3.0              702.0           0.0   \n\n         ...  target_lag_4  target_item_lag_4  target_shop_lag_4  \\\n0        ...           0.0                0.0                0.0   \n1        ...           0.0                0.0                0.0   \n2        ...           0.0                0.0                0.0   \n3        ...           0.0                0.0                0.0   \n4        ...           0.0                3.0             7827.0   \n...      ...           ...                ...                ...   \n6639289  ...           0.0               12.0              675.0   \n6639290  ...           0.0                0.0                0.0   \n6639291  ...           0.0                4.0              675.0   \n6639292  ...           0.0                2.0              675.0   \n6639293  ...           0.0                2.0              675.0   \n\n         target_lag_5  target_item_lag_5  target_shop_lag_5  target_lag_12  \\\n0                 0.0                0.0                0.0            0.0   \n1                 0.0                0.0                0.0            0.0   \n2                 0.0                0.0                0.0            0.0   \n3                 0.0                0.0                0.0            0.0   \n4                 0.0               10.0             7792.0            0.0   \n...               ...                ...                ...            ...   \n6639289           0.0               19.0              622.0            0.0   \n6639290           0.0                0.0                0.0            0.0   \n6639291           0.0                8.0              622.0            0.0   \n6639292           0.0                4.0              622.0            0.0   \n6639293           0.0                2.0              622.0            0.0   \n\n         target_item_lag_12  target_shop_lag_12  item_category_id  \n0                       0.0                 0.0                37  \n1                       0.0                 0.0                38  \n2                       0.0                 0.0                40  \n3                       0.0                 0.0                37  \n4                       0.0                 0.0                57  \n...                     ...                 ...               ...  \n6639289                 0.0                 0.0                55  \n6639290                 0.0                 0.0                64  \n6639291                 9.0              1251.0                55  \n6639292                 0.0                 0.0                40  \n6639293                 6.0              1251.0                37  \n\n[6639294 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>shop_id</th>\n      <th>item_id</th>\n      <th>date_block_num</th>\n      <th>target</th>\n      <th>target_shop</th>\n      <th>target_item</th>\n      <th>target_lag_1</th>\n      <th>target_item_lag_1</th>\n      <th>target_shop_lag_1</th>\n      <th>target_lag_2</th>\n      <th>...</th>\n      <th>target_lag_4</th>\n      <th>target_item_lag_4</th>\n      <th>target_shop_lag_4</th>\n      <th>target_lag_5</th>\n      <th>target_item_lag_5</th>\n      <th>target_shop_lag_5</th>\n      <th>target_lag_12</th>\n      <th>target_item_lag_12</th>\n      <th>target_shop_lag_12</th>\n      <th>item_category_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>54</td>\n      <td>10297</td>\n      <td>12</td>\n      <td>4.0</td>\n      <td>8198.0</td>\n      <td>23.0</td>\n      <td>3.0</td>\n      <td>42.0</td>\n      <td>10055.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>54</td>\n      <td>10296</td>\n      <td>12</td>\n      <td>3.0</td>\n      <td>8198.0</td>\n      <td>17.0</td>\n      <td>0.0</td>\n      <td>24.0</td>\n      <td>10055.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>54</td>\n      <td>10298</td>\n      <td>12</td>\n      <td>14.0</td>\n      <td>8198.0</td>\n      <td>182.0</td>\n      <td>21.0</td>\n      <td>369.0</td>\n      <td>10055.0</td>\n      <td>119.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>54</td>\n      <td>10300</td>\n      <td>12</td>\n      <td>3.0</td>\n      <td>8198.0</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>54.0</td>\n      <td>10055.0</td>\n      <td>31.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>10284</td>\n      <td>12</td>\n      <td>1.0</td>\n      <td>8198.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>10055.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>7827.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>7792.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6639289</th>\n      <td>45</td>\n      <td>18454</td>\n      <td>34</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>702.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>675.0</td>\n      <td>0.0</td>\n      <td>19.0</td>\n      <td>622.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>6639290</th>\n      <td>45</td>\n      <td>16188</td>\n      <td>34</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>702.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>6639291</th>\n      <td>45</td>\n      <td>15757</td>\n      <td>34</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>702.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>675.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>622.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>1251.0</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>6639292</th>\n      <td>45</td>\n      <td>19648</td>\n      <td>34</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>702.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>675.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>622.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>6639293</th>\n      <td>45</td>\n      <td>969</td>\n      <td>34</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>702.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>675.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>622.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>1251.0</td>\n      <td>37</td>\n    </tr>\n  </tbody>\n</table>\n<p>6639294 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"dates = all_data['date_block_num']\n\nlast_block = dates.max()\n#print(last_block)\n\nX_train = all_data.loc[dates < last_block - 1].drop(to_drop_cols, axis=1)\nX_cv = all_data.loc[dates ==  last_block - 1].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[dates <  last_block - 1, 'target'].values\ny_cv = all_data.loc[dates ==  last_block - 1, 'target'].values\ny_test =  all_data.loc[dates == last_block, 'target'].values\n\n# print(X_train_part.shape)\n# print(y_train_part.shape)\n\n# X_train.to_pickle(\"X_train.pkl\")\n# X_train_part.to_pickle(\"X_train_part.pkl\")\n# X_test.to_pickle(\"X_test.pkl\")\n\n# np.save(\"y_train.npy\", y_train)\n# np.save(\"y_train_part.npy\", y_train_part)\n# np.save(\"y_test.npy\", y_test)","execution_count":134,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgbm_train(X_train,y_train,X_test,y_test,num_iter):\n    lgb_params = {\n                   'feature_fraction': 0.75,\n                   'metric': 'rmse',\n                   'nthread':1, \n                   'min_data_in_leaf': 2**7, \n                   'bagging_fraction': 0.75, \n                   'learning_rate': 0.03, \n                   'objective': 'mse', \n                   'bagging_seed': 2**7, \n                   'num_leaves': 2**7,\n                   'bagging_freq':1,\n                   'verbose':0 \n                  }\n\n    evals_result = {}\n    \n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_valid = lgb.Dataset(X_test, label=y_test)\n    model = lgb.train(lgb_params, lgb_train, num_iter,\n                      valid_sets=[lgb_train, lgb_valid],\n                      evals_result=evals_result,\n                      verbose_eval=10)\n    return model,evals_result\n","execution_count":135,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y_train_clip = y_train.clip(0,20)\nmodel,evals_result  = lgbm_train(X_train, y_train_clip, X_cv, y_cv, 200)","execution_count":136,"outputs":[{"output_type":"stream","text":"(6186922, 21)\n(6186922,)\n[10]\ttraining's rmse: 1.0627\tvalid_1's rmse: 5.29971\n[20]\ttraining's rmse: 0.983723\tvalid_1's rmse: 5.27079\n[30]\ttraining's rmse: 0.935232\tvalid_1's rmse: 5.25121\n[40]\ttraining's rmse: 0.904167\tvalid_1's rmse: 5.23739\n[50]\ttraining's rmse: 0.88396\tvalid_1's rmse: 5.22687\n[60]\ttraining's rmse: 0.870733\tvalid_1's rmse: 5.21974\n[70]\ttraining's rmse: 0.861056\tvalid_1's rmse: 5.2142\n[80]\ttraining's rmse: 0.854165\tvalid_1's rmse: 5.20985\n[90]\ttraining's rmse: 0.848822\tvalid_1's rmse: 5.20624\n[100]\ttraining's rmse: 0.843917\tvalid_1's rmse: 5.20335\n[110]\ttraining's rmse: 0.840232\tvalid_1's rmse: 5.20135\n[120]\ttraining's rmse: 0.836431\tvalid_1's rmse: 5.19959\n[130]\ttraining's rmse: 0.832962\tvalid_1's rmse: 5.19848\n[140]\ttraining's rmse: 0.829669\tvalid_1's rmse: 5.19733\n[150]\ttraining's rmse: 0.826843\tvalid_1's rmse: 5.19628\n[160]\ttraining's rmse: 0.824436\tvalid_1's rmse: 5.19543\n[170]\ttraining's rmse: 0.822381\tvalid_1's rmse: 5.19466\n[180]\ttraining's rmse: 0.820437\tvalid_1's rmse: 5.19386\n[190]\ttraining's rmse: 0.818377\tvalid_1's rmse: 5.19297\n[200]\ttraining's rmse: 0.816905\tvalid_1's rmse: 5.19253\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_model('lgbm_model_full_clip20.txt')","execution_count":137,"outputs":[{"output_type":"execute_result","execution_count":137,"data":{"text/plain":"<lightgbm.basic.Booster at 0x7f3b1e25d0f0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = lgb.Booster(model_file='/kaggle/input/predict-future-sales/lgbm_model.txt')","execution_count":138,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_clip = y_train.clip(0,20)","execution_count":139,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regression in preparation for ensembling\nlr = LinearRegression()\n\nlr.fit(X_train.values, y_train_clip)\npred_lr = lr.predict(X_cv)\n\nprint('Test RMSE for linreg is %f' % np.sqrt(mean_squared_error(y_cv, pred_lr)))","execution_count":140,"outputs":[{"output_type":"stream","text":"Test RMSE for linreg is 5.229523\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb_cv = model.predict(X_cv)\nlr.fit(X_train.values, y_train_clip)\npred_lr_cv = lr.predict(X_cv)\n#Clip predictions for ensembling\npred_lr_cv = pred_lr_cv.clip(0,20)\npred_lgb_cv = pred_lgb_cv.clip(0,20)\nX_cv_level2 = np.c_[pred_lr_cv, pred_lgb_cv]\nnp.save(\"X_cv_level2.npy\", X_cv_level2)","execution_count":141,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb_test = model.predict(X_test)\nlr.fit(X_train.values, y_train_clip)\npred_lr_test = lr.predict(X_test)\npred_lr_test = pred_lr_test.clip(0,20)\npred_lgb_test = pred_lgb_test.clip(0,20)\nX_test_level2 = np.c_[pred_lr_test, pred_lgb_test]\nnp.save(\"X_test_level2.npy\", X_test_level2)","execution_count":142,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First level predictions for ensembling\n\ndates_train = dates[dates <  last_block - 1]\ndates_train_level2 = dates_train[dates_train.isin([28, 29, 30, 31, 32])]\n\n# That is how we get target for the 2nd level dataset\ny_train_level2 = y_train[dates_train.isin([28, 29, 30, 31, 32])]\n\n# And here we create 2nd level feeature matrix, init it with zeros first\nX_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n\ncount = 0\n# Now fill `X_train_level2` with metafeatures\nfor cur_block_num in [28, 29, 30, 31, 32]:\n    \n    print(cur_block_num)\n    \n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit LightGBM and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    '''\n    X_train_cur = X_train[dates_train.isin(range(12,cur_block_num))]\n    y_train_cur = y_train_clip[dates_train.isin(range(12,cur_block_num))]\n    X_test_cur = X_train[dates_train.isin([cur_block_num])]\n    lr_temp = LinearRegression()\n    lr_temp.fit(X_train_cur.values, y_train_cur)\n    pred_lr_temp = lr_temp.predict(X_test_cur.values)\n    \n    model_temp,_ = lgbm_train(X_train_cur, y_train_cur, X_train_cur, y_train_cur, 200)\n    pred_lgb_temp = model_temp.predict(X_test_cur)\n    \n    for i in range(len(pred_lr_temp)):\n        X_train_level2[count+i][0]=pred_lr_temp[i]\n        X_train_level2[count+i][1]=pred_lgb_temp[i]\n    count+=len(pred_lr_temp)\nnp.save(\"X_train_full_level2.npy\", X_train_level2)\nnp.save(\"y_train_full_level2.npy\", y_train_level2)","execution_count":145,"outputs":[{"output_type":"stream","text":"28\n[10]\ttraining's rmse: 1.08358\tvalid_1's rmse: 1.08358\n[20]\ttraining's rmse: 1.00086\tvalid_1's rmse: 1.00086\n[30]\ttraining's rmse: 0.950159\tvalid_1's rmse: 0.950159\n[40]\ttraining's rmse: 0.91737\tvalid_1's rmse: 0.91737\n[50]\ttraining's rmse: 0.895866\tvalid_1's rmse: 0.895866\n[60]\ttraining's rmse: 0.881815\tvalid_1's rmse: 0.881815\n[70]\ttraining's rmse: 0.871556\tvalid_1's rmse: 0.871556\n[80]\ttraining's rmse: 0.863844\tvalid_1's rmse: 0.863844\n[90]\ttraining's rmse: 0.857774\tvalid_1's rmse: 0.857774\n[100]\ttraining's rmse: 0.852633\tvalid_1's rmse: 0.852633\n[110]\ttraining's rmse: 0.848298\tvalid_1's rmse: 0.848298\n[120]\ttraining's rmse: 0.844331\tvalid_1's rmse: 0.844331\n[130]\ttraining's rmse: 0.840536\tvalid_1's rmse: 0.840536\n[140]\ttraining's rmse: 0.837106\tvalid_1's rmse: 0.837106\n[150]\ttraining's rmse: 0.833922\tvalid_1's rmse: 0.833922\n[160]\ttraining's rmse: 0.831191\tvalid_1's rmse: 0.831191\n[170]\ttraining's rmse: 0.828993\tvalid_1's rmse: 0.828993\n[180]\ttraining's rmse: 0.826931\tvalid_1's rmse: 0.826931\n[190]\ttraining's rmse: 0.825012\tvalid_1's rmse: 0.825012\n[200]\ttraining's rmse: 0.823216\tvalid_1's rmse: 0.823216\n29\n[10]\ttraining's rmse: 1.07952\tvalid_1's rmse: 1.07952\n[20]\ttraining's rmse: 0.997688\tvalid_1's rmse: 0.997688\n[30]\ttraining's rmse: 0.947468\tvalid_1's rmse: 0.947468\n[40]\ttraining's rmse: 0.915109\tvalid_1's rmse: 0.915109\n[50]\ttraining's rmse: 0.893849\tvalid_1's rmse: 0.893849\n[60]\ttraining's rmse: 0.879969\tvalid_1's rmse: 0.879969\n[70]\ttraining's rmse: 0.869971\tvalid_1's rmse: 0.869971\n[80]\ttraining's rmse: 0.862573\tvalid_1's rmse: 0.862573\n[90]\ttraining's rmse: 0.856425\tvalid_1's rmse: 0.856425\n[100]\ttraining's rmse: 0.851554\tvalid_1's rmse: 0.851554\n[110]\ttraining's rmse: 0.847189\tvalid_1's rmse: 0.847189\n[120]\ttraining's rmse: 0.843281\tvalid_1's rmse: 0.843281\n[130]\ttraining's rmse: 0.839988\tvalid_1's rmse: 0.839988\n[140]\ttraining's rmse: 0.836354\tvalid_1's rmse: 0.836354\n[150]\ttraining's rmse: 0.833443\tvalid_1's rmse: 0.833443\n[160]\ttraining's rmse: 0.830892\tvalid_1's rmse: 0.830892\n[170]\ttraining's rmse: 0.828751\tvalid_1's rmse: 0.828751\n[180]\ttraining's rmse: 0.826685\tvalid_1's rmse: 0.826685\n[190]\ttraining's rmse: 0.824757\tvalid_1's rmse: 0.824757\n[200]\ttraining's rmse: 0.822851\tvalid_1's rmse: 0.822851\n30\n[10]\ttraining's rmse: 1.07414\tvalid_1's rmse: 1.07414\n[20]\ttraining's rmse: 0.992382\tvalid_1's rmse: 0.992382\n[30]\ttraining's rmse: 0.942384\tvalid_1's rmse: 0.942384\n[40]\ttraining's rmse: 0.910038\tvalid_1's rmse: 0.910038\n[50]\ttraining's rmse: 0.888893\tvalid_1's rmse: 0.888893\n[60]\ttraining's rmse: 0.875129\tvalid_1's rmse: 0.875129\n[70]\ttraining's rmse: 0.86492\tvalid_1's rmse: 0.86492\n[80]\ttraining's rmse: 0.857517\tvalid_1's rmse: 0.857517\n[90]\ttraining's rmse: 0.851498\tvalid_1's rmse: 0.851498\n[100]\ttraining's rmse: 0.846509\tvalid_1's rmse: 0.846509\n[110]\ttraining's rmse: 0.842139\tvalid_1's rmse: 0.842139\n[120]\ttraining's rmse: 0.838246\tvalid_1's rmse: 0.838246\n[130]\ttraining's rmse: 0.83499\tvalid_1's rmse: 0.83499\n[140]\ttraining's rmse: 0.831531\tvalid_1's rmse: 0.831531\n[150]\ttraining's rmse: 0.828703\tvalid_1's rmse: 0.828703\n[160]\ttraining's rmse: 0.826197\tvalid_1's rmse: 0.826197\n[170]\ttraining's rmse: 0.824071\tvalid_1's rmse: 0.824071\n[180]\ttraining's rmse: 0.821991\tvalid_1's rmse: 0.821991\n[190]\ttraining's rmse: 0.820187\tvalid_1's rmse: 0.820187\n[200]\ttraining's rmse: 0.818422\tvalid_1's rmse: 0.818422\n31\n[10]\ttraining's rmse: 1.06734\tvalid_1's rmse: 1.06734\n[20]\ttraining's rmse: 0.986415\tvalid_1's rmse: 0.986415\n[30]\ttraining's rmse: 0.936697\tvalid_1's rmse: 0.936697\n[40]\ttraining's rmse: 0.904861\tvalid_1's rmse: 0.904861\n[50]\ttraining's rmse: 0.884001\tvalid_1's rmse: 0.884001\n[60]\ttraining's rmse: 0.870478\tvalid_1's rmse: 0.870478\n[70]\ttraining's rmse: 0.860667\tvalid_1's rmse: 0.860667\n[80]\ttraining's rmse: 0.853518\tvalid_1's rmse: 0.853518\n[90]\ttraining's rmse: 0.847604\tvalid_1's rmse: 0.847604\n[100]\ttraining's rmse: 0.842619\tvalid_1's rmse: 0.842619\n[110]\ttraining's rmse: 0.838552\tvalid_1's rmse: 0.838552\n[120]\ttraining's rmse: 0.834769\tvalid_1's rmse: 0.834769\n[130]\ttraining's rmse: 0.831297\tvalid_1's rmse: 0.831297\n[140]\ttraining's rmse: 0.828138\tvalid_1's rmse: 0.828138\n[150]\ttraining's rmse: 0.825131\tvalid_1's rmse: 0.825131\n[160]\ttraining's rmse: 0.822549\tvalid_1's rmse: 0.822549\n[170]\ttraining's rmse: 0.820395\tvalid_1's rmse: 0.820395\n[180]\ttraining's rmse: 0.818443\tvalid_1's rmse: 0.818443\n[190]\ttraining's rmse: 0.816695\tvalid_1's rmse: 0.816695\n[200]\ttraining's rmse: 0.815054\tvalid_1's rmse: 0.815054\n32\n[10]\ttraining's rmse: 1.06351\tvalid_1's rmse: 1.06351\n[20]\ttraining's rmse: 0.982873\tvalid_1's rmse: 0.982873\n[30]\ttraining's rmse: 0.933297\tvalid_1's rmse: 0.933297\n[40]\ttraining's rmse: 0.901686\tvalid_1's rmse: 0.901686\n[50]\ttraining's rmse: 0.881\tvalid_1's rmse: 0.881\n[60]\ttraining's rmse: 0.86745\tvalid_1's rmse: 0.86745\n[70]\ttraining's rmse: 0.85754\tvalid_1's rmse: 0.85754\n[80]\ttraining's rmse: 0.850496\tvalid_1's rmse: 0.850496\n[90]\ttraining's rmse: 0.844759\tvalid_1's rmse: 0.844759\n[100]\ttraining's rmse: 0.840233\tvalid_1's rmse: 0.840233\n[110]\ttraining's rmse: 0.836423\tvalid_1's rmse: 0.836423\n[120]\ttraining's rmse: 0.83263\tvalid_1's rmse: 0.83263\n[130]\ttraining's rmse: 0.829205\tvalid_1's rmse: 0.829205\n[140]\ttraining's rmse: 0.825749\tvalid_1's rmse: 0.825749\n[150]\ttraining's rmse: 0.823251\tvalid_1's rmse: 0.823251\n[160]\ttraining's rmse: 0.820929\tvalid_1's rmse: 0.820929\n[170]\ttraining's rmse: 0.818661\tvalid_1's rmse: 0.818661\n[180]\ttraining's rmse: 0.816395\tvalid_1's rmse: 0.816395\n[190]\ttraining's rmse: 0.814557\tvalid_1's rmse: 0.814557\n[200]\ttraining's rmse: 0.812752\tvalid_1's rmse: 0.812752\n33\n[10]\ttraining's rmse: 1.06297\tvalid_1's rmse: 1.06297\n[20]\ttraining's rmse: 0.982795\tvalid_1's rmse: 0.982795\n[30]\ttraining's rmse: 0.933585\tvalid_1's rmse: 0.933585\n[40]\ttraining's rmse: 0.901946\tvalid_1's rmse: 0.901946\n[50]\ttraining's rmse: 0.881238\tvalid_1's rmse: 0.881238\n[60]\ttraining's rmse: 0.867942\tvalid_1's rmse: 0.867942\n[70]\ttraining's rmse: 0.857954\tvalid_1's rmse: 0.857954\n[80]\ttraining's rmse: 0.850646\tvalid_1's rmse: 0.850646\n[90]\ttraining's rmse: 0.844865\tvalid_1's rmse: 0.844865\n[100]\ttraining's rmse: 0.839959\tvalid_1's rmse: 0.839959\n[110]\ttraining's rmse: 0.83615\tvalid_1's rmse: 0.83615\n[120]\ttraining's rmse: 0.832414\tvalid_1's rmse: 0.832414\n[130]\ttraining's rmse: 0.829149\tvalid_1's rmse: 0.829149\n[140]\ttraining's rmse: 0.825837\tvalid_1's rmse: 0.825837\n[150]\ttraining's rmse: 0.823013\tvalid_1's rmse: 0.823013\n[160]\ttraining's rmse: 0.820613\tvalid_1's rmse: 0.820613\n[170]\ttraining's rmse: 0.818294\tvalid_1's rmse: 0.818294\n[180]\ttraining's rmse: 0.816221\tvalid_1's rmse: 0.816221\n[190]\ttraining's rmse: 0.814291\tvalid_1's rmse: 0.814291\n[200]\ttraining's rmse: 0.812504\tvalid_1's rmse: 0.812504\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test RMSE for linreg is %f' % np.sqrt(mean_squared_error(y_cv, pred_lr_cv)))\nprint('Test RMSE for lgbm is %f' % np.sqrt(mean_squared_error(y_cv, pred_lgb_cv)))","execution_count":147,"outputs":[{"output_type":"stream","text":"Test RMSE for linreg is 5.247907\nTest RMSE for lgbm is 5.193346\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Ensembling with averaging implementation\n\nfrom sklearn.metrics import r2_score\n#X_train_level2[:,0].shape\nprint(r2_score(y_train_level2.clip(0,20), X_train_level2[:,0]))\nprint(r2_score(y_train_level2.clip(0,20), X_train_level2[:,1]))\nalphas_to_try = np.linspace(0, 1, 1001)\nbest_r2 = 0.0\nbest_alpha = -1\nfor alpha in alphas_to_try:\n    curr_r2 = r2_score(y_train_level2.clip(0,20), np.dot(X_train_level2, [alpha,1-alpha]))\n    if curr_r2 > best_r2:\n        best_r2 = curr_r2\n        best_alpha = alpha\n# YOUR CODE GOES HERE\nbest_alpha = best_alpha\nr2_train_simple_mix = best_r2\n\nprint('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))","execution_count":180,"outputs":[{"output_type":"stream","text":"0.1638807190308701\n0.4273387216055792\nBest alpha: 0.000000; Corresponding r2 score on train: 0.427339\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ensembling with stacking predictions\nX_train_level2 = np.load(\"X_train_full_level2.npy\")\ny_train_level2 = np.load(\"y_train_full_level2.npy\")\n\n\nlr.fit(X_train_level2.clip(0,20), y_train_level2.clip(0,20))\n\ncv_preds = lr.predict(X_cv_level2)\nrmse_test_stacking = np.sqrt(mean_squared_error(y_cv, cv_preds))\n\ntest_preds = lr.predict(X_test_level2)\nrmse_test_stacking","execution_count":159,"outputs":[{"output_type":"execute_result","execution_count":159,"data":{"text/plain":"5.194295736604871"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions on the test set\nmerged = test.copy()\nmerged['item_cnt_month'] = test_preds\nmerged['item_cnt_month']=merged['item_cnt_month'].clip(lower=0,upper=20)\nmerged=merged.drop(['shop_id','item_id'],axis=1)\nmerged['ID']=merged['ID'].astype('int')\nmerged.to_csv(\"lightgbm_clipto20_ens_full_l2full.csv\",index=False)\nprint(merged.head)","execution_count":158,"outputs":[{"output_type":"stream","text":"<bound method NDFrame.head of             ID  item_cnt_month\n0            0        0.454706\n1            1        0.229680\n2            2        0.810350\n3            3        0.321987\n4            4        1.631143\n...        ...             ...\n214195  214195        0.138287\n214196  214196        0.067720\n214197  214197        0.046541\n214198  214198        0.043305\n214199  214199        0.043210\n\n[214200 rows x 2 columns]>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(\"X_test.npy\", X_test)","execution_count":182,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code to run to get the submitted result\n\nmodel_lgb = lgb.Booster(model_file='lgbm_model_full_clip20.txt')\n\nX_test_fin = np.load('X_test.npy')\nlgb_pred_test_fin = model_lgb.predict(X_test_fin)\nlgb_pred_test_fin = lgb_pred_test_fin.clip(0,20)\n\ntest_fin = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\ntest_fin['item_cnt_month'] = test_preds\ntest_fin['item_cnt_month']=merged['item_cnt_month'].clip(lower=0,upper=20)\ntest_fin = test_fin.drop(['shop_id','item_id'],axis=1)\ntest_fin['ID'] = test_fin['ID'].astype('int')\ntest_fin.to_csv(\"final_submission.csv\",index=False)\nprint(test_fin.head)","execution_count":191,"outputs":[{"output_type":"stream","text":"<bound method NDFrame.head of             ID  item_cnt_month\n0            0        0.454706\n1            1        0.229680\n2            2        0.810350\n3            3        0.321987\n4            4        1.631143\n...        ...             ...\n214195  214195        0.138287\n214196  214196        0.067720\n214197  214197        0.046541\n214198  214198        0.043305\n214199  214199        0.043210\n\n[214200 rows x 2 columns]>\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}