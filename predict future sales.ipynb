{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nfrom itertools import product\n\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"item_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nsales_train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\nsample_submission = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales_train\n\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\nall_data2 = all_data.copy()\ndel grid, gb \ngc.collect();","execution_count":11,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/generic.py:1455: FutureWarning: using a dict with renaming is deprecated and will be removed\nin a future version.\n\nFor column-specific groupby renaming, use named aggregation\n\n    >>> df.groupby(...).agg(name=('column', aggfunc))\n\n  return super().aggregate(arg, *args, **kwargs)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\nall_data = downcast_dtypes(all_data)\ngc.collect();","execution_count":12,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  \n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=6), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee67b2438ed3487fbf82cab80bf879a7"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"dates = all_data['date_block_num']\n\nlast_block = dates.max()\n\nX_train = all_data.loc[dates <=  last_block].drop(to_drop_cols, axis=1)\nX_train_part = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[dates <=  last_block, 'target'].values\ny_train_part = all_data.loc[dates <  last_block, 'target'].values\ny_test =  all_data.loc[dates == last_block, 'target'].values\n\n# print(X_train_part.shape)\n# print(y_train_part.shape)\n\nX_train.to_pickle(\"X_train.pkl\")\nX_train_part.to_pickle(\"X_train_part.pkl\")\nX_test.to_pickle(\"X_test.pkl\")\n\nnp.save(\"y_train.npy\", y_train)\nnp.save(\"y_train_part.npy\", y_train_part)\nnp.save(\"y_test.npy\", y_test)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data2.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\ntest_data = test.copy()\ntest_data['date_block_num'] = 34\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data2[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    test_data = pd.merge(test_data, train_shift, on=index_cols, how='left').fillna(0)\n\ntest_data = pd.merge(test_data, item_category_mapping, how='left', on='item_id')\ntest_data = downcast_dtypes(test_data)\ngc.collect();\n","execution_count":14,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  if __name__ == '__main__':\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=6), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf14756ad4a4af0973779e504ea7613"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgbm_train(X_train,y_train,X_test,y_test,num_iter):\n    lgb_params = {\n                   'feature_fraction': 0.75,\n                   'metric': 'rmse',\n                   'nthread':1, \n                   'min_data_in_leaf': 2**7, \n                   'bagging_fraction': 0.75, \n                   'learning_rate': 0.03, \n                   'objective': 'mse', \n                   'bagging_seed': 2**7, \n                   'num_leaves': 2**7,\n                   'bagging_freq':1,\n                   'verbose':0 \n                  }\n\n    evals_result = {}\n    \n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_valid = lgb.Dataset(X_test, label=y_test)\n    model = lgb.train(lgb_params, lgb_train, num_iter,\n                      valid_sets=[lgb_train, lgb_valid],\n                      evals_result=evals_result,\n                      verbose_eval=10)\n    return model,evals_result\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = pd.read_pickle(\"/kaggle/input/predict-future-sales/X_train.pkl\")\nX_train_part = pd.read_pickle(\"/kaggle/input/predict-future-sales/X_train_part.pkl\")\nX_test =  pd.read_pickle(\"/kaggle/input/predict-future-sales/X_test.pkl\")\n\ny_train = np.load(\"/kaggle/input/predict-future-sales/y_train.npy\")\ny_train_part = np.load(\"/kaggle/input/predict-future-sales/y_train_part.npy\")\ny_test =  np.load(\"/kaggle/input/predict-future-sales/y_test.npy\")\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_part = y_train_part.clip(0,40)\n\nprint(X_train_part.shape)\nprint(y_train_part.shape)\nmodel,evals_result  = lgbm_train(X_train_part, y_train_part, X_test, y_test, 200)","execution_count":null,"outputs":[{"output_type":"stream","text":"(6186922, 21)\n(6186922,)\n[10]\ttraining's rmse: 1.28027\tvalid_1's rmse: 5.27475\n[20]\ttraining's rmse: 1.17893\tvalid_1's rmse: 5.22955\n[30]\ttraining's rmse: 1.11709\tvalid_1's rmse: 5.2001\n[40]\ttraining's rmse: 1.07662\tvalid_1's rmse: 5.18031\n[50]\ttraining's rmse: 1.05003\tvalid_1's rmse: 5.16596\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full model traininig for submission\ny_train = y_train.clip(0,40)\n\nprint(X_train.shape)\nprint(y_train.shape)\nmodel_full, evals_result  = lgbm_train(X_train, y_train, X_test, y_test, 200) ","execution_count":22,"outputs":[{"output_type":"stream","text":"(6425094, 21)\n(6425094,)\n[10]\ttraining's rmse: 1.28088\tvalid_1's rmse: 5.27328\n[20]\ttraining's rmse: 1.18107\tvalid_1's rmse: 5.2259\n[30]\ttraining's rmse: 1.12012\tvalid_1's rmse: 5.19518\n[40]\ttraining's rmse: 1.08018\tvalid_1's rmse: 5.17332\n[50]\ttraining's rmse: 1.05399\tvalid_1's rmse: 5.15705\n[60]\ttraining's rmse: 1.03723\tvalid_1's rmse: 5.14567\n[70]\ttraining's rmse: 1.02489\tvalid_1's rmse: 5.13689\n[80]\ttraining's rmse: 1.0156\tvalid_1's rmse: 5.13003\n[90]\ttraining's rmse: 1.00835\tvalid_1's rmse: 5.12419\n[100]\ttraining's rmse: 1.00199\tvalid_1's rmse: 5.12054\n[110]\ttraining's rmse: 0.997315\tvalid_1's rmse: 5.11699\n[120]\ttraining's rmse: 0.992634\tvalid_1's rmse: 5.11384\n[130]\ttraining's rmse: 0.988291\tvalid_1's rmse: 5.11204\n[140]\ttraining's rmse: 0.984292\tvalid_1's rmse: 5.10975\n[150]\ttraining's rmse: 0.980693\tvalid_1's rmse: 5.1078\n[160]\ttraining's rmse: 0.97772\tvalid_1's rmse: 5.10654\n[170]\ttraining's rmse: 0.974991\tvalid_1's rmse: 5.10579\n[180]\ttraining's rmse: 0.972717\tvalid_1's rmse: 5.10459\n[190]\ttraining's rmse: 0.970448\tvalid_1's rmse: 5.10377\n[200]\ttraining's rmse: 0.968711\tvalid_1's rmse: 5.10282\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_model('lgbm_model.txt')\nmodel_full.save_model('lgbm_model_full.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.Booster(model_file='/kaggle/input/predict-future-sales/lgbm_model.txt')\nmodel_full = lgb.Booster(model_file='/kaggle/input/predict-future-sales/lgbm_model_full.txt')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regression in preparation for ensembling\nlr = LinearRegression()\ny_train_part = y_train_part.clip(0,40)\nlr.fit(X_train_part.values, y_train_part)\nprint(X_test.shape)\nprint(y_test.shape)\npred_lr = lr.predict(X_test)\nprint('Test RMSE for linreg is %f' % np.sqrt(mean_squared_error(y_test, pred_lr)))\n","execution_count":20,"outputs":[{"output_type":"stream","text":"(238172, 21)\n(238172,)\nTest RMSE for linreg is 5.181381\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb = model.predict(X_test)\nX_test_level2 = np.c_[pred_lr, pred_lgb]\nnp.save(\"X_test_level2.npy\", X_test_level2)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndates_train = dates[dates <  last_block]\ndates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n\n# That is how we get target for the 2nd level dataset\ny_train_level2 = y_train_part[dates_train.isin([27, 28, 29, 30, 31, 32])]\n\n# And here we create 2nd level feeature matrix, init it with zeros first\nX_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n\ncount = 0\n# Now fill `X_train_level2` with metafeatures\nfor cur_block_num in [27, 28, 29, 30, 31, 32]:\n    \n    print(cur_block_num)\n    \n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit LightGBM and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    '''\n    X_train_cur = X_train_part[dates_train.isin(range(12,cur_block_num))]\n    y_train_cur = y_train_part[dates_train.isin(range(12,cur_block_num))]\n    X_test_cur = X_train_part[dates_train.isin([cur_block_num])]\n    \n    lr.fit(X_train_cur.values, y_train_cur)\n    pred_lr = lr.predict(X_test_cur.values)\n    \n    model_temp,_ = lgbm_train(X_train_cur, y_train_cur, X_train_cur, y_train_cur, 200)\n    pred_lgb = model_temp.predict(X_test_cur)\n    \n    for i in range(len(pred_lr)):\n        X_train_level2[count+i][0]=pred_lr[i]\n        X_train_level2[count+i][1]=pred_lgb[i]\n    count+=len(pred_lr)\nnp.save(\"X_train_full_level2.npy\", X_train_level2)\nnp.save(\"y_train_full_level2.npy\", y_train_level2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Full linear regression\nlr_full = LinearRegression()\ny_train = y_train.clip(0,40)\nlr_full.fit(X_train.values, y_train)\npred_lr_full = lr_full.predict(X_test)\nprint('Test RMSE for linreg is %f' % np.sqrt(mean_squared_error(y_test, pred_lr_full)))","execution_count":25,"outputs":[{"output_type":"stream","text":"Test RMSE for linreg is 5.229523\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas_to_try = np.linspace(0, 1, 1001)\nbest_rmse = 1000\nbest_alpha = -1\nfor alpha in alphas_to_try:\n    curr_rmse = np.sqrt(mean_squared_error(y_train_level2, np.dot(X_train_level2, [alpha,1-alpha])))\n    if curr_rmse < best_rmse:\n        best_rmse = curr_rmse\n        best_alpha = alpha\n# YOUR CODE GOES HERE\nbest_alpha = best_alpha\nprint(best_rmse)\nprint(best_alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_level2 = np.load(\"/kaggle/input/predict-future-sales/X_train_level2.npy\")\ny_train_level2 = np.load(\"/kaggle/input/predict-future-sales/y_train_level2.npy\")\n\nlr.fit(X_train_level2, y_train_level2)\n\ntest_preds = lr.predict(X_test_level2)\nrmse_test_stacking = np.sqrt(mean_squared_error(y_test, test_preds))\nrmse_test_stacking","execution_count":16,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/predict-future-sales/X_train_level2.npy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-3d50776b881e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_level2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/predict-future-sales/X_train_level2.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train_level2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/predict-future-sales/y_train_level2.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_level2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_level2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/predict-future-sales/X_train_level2.npy'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb_test = model.predict(X_test)\npred_ens_test = best_alpha*pred_lgb_test + (1-best_alpha)*pred_lr\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop_cols = ['ID','date_block_num']\nX_test_final =  test_data.drop(to_drop_cols, axis=1)\npred_lgb_final = model_full.predict(X_test_final)\n# pred_lr_final = lr_full.predict(X_test_final)\n#pred_ens = best_alpha*pred_lgb_final + (1-best_alpha)*pred_lr_final\n#print(pred_lgb)\n\nmerged = test.copy()\nmerged['item_cnt_month'] = pred_lgb_final\nmerged['item_cnt_month']=merged['item_cnt_month'].clip(lower=0,upper=20)\nmerged=merged.drop(['shop_id','item_id'],axis=1)\nmerged['ID']=merged['ID'].astype('int')\nmerged.to_csv(\"lightgbm_clipto40.csv\",index=False)\nprint(merged.head)","execution_count":27,"outputs":[{"output_type":"stream","text":"<bound method NDFrame.head of             ID  item_cnt_month\n0            0        0.430201\n1            1        0.195441\n2            2        0.827994\n3            3        0.288239\n4            4        1.906440\n...        ...             ...\n214195  214195        0.124841\n214196  214196        0.045553\n214197  214197        0.021552\n214198  214198        0.019733\n214199  214199        0.019733\n\n[214200 rows x 2 columns]>\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}