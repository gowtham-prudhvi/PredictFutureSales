{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nfrom itertools import product\n\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nitem_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nsales_train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\nsample_submission = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oct_trans=sales_train[sales_train[\"date\"].str.endswith(\"10.2015\")]\noct_sales = oct_trans.groupby(['shop_id','item_id'])['item_cnt_day'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nov_trans=sales_train[sales_train[\"date\"].str.endswith(\"11.2014\")]\nnov_sales = nov_trans.groupby(['shop_id','item_id'])['item_cnt_day'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged=pd.merge(test,oct_sales,on=['shop_id','item_id'],how=\"outer\")\n#merged=merged[merged['ID']!=np.nan]\nmerged=merged.dropna(subset=['ID'])\nmerged=merged.replace(np.nan,0)\nmerged['item_cnt_month']=merged['item_cnt_day'].clip(lower=0,upper=20)\nmerged=merged.drop(['shop_id','item_id','item_cnt_day'],axis=1)\nmerged['ID']=merged['ID'].astype('int')\nmerged.to_csv(\"october_sales.csv\",index=False)\nmerged\n# print(merged.shape)\n# sample_submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales_train\n\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\nall_data2 = all_data.copy()\ndel grid, gb \ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\nall_data = downcast_dtypes(all_data)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"dates = all_data['date_block_num']\n\nlast_block = dates.max()\nprint(last_block)\nX_train = all_data.loc[dates <=  last_block].drop(to_drop_cols, axis=1)\n#print(X_train.head)\n#X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n#print(X_test.head)\n\ny_train = all_data.loc[dates <=  last_block, 'target'].values\n#y_test =  all_data.loc[dates == last_block, 'target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data2.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\ntest_data = test.copy()\ntest_data['date_block_num'] = 34\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data2[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    test_data = pd.merge(test_data, train_shift, on=index_cols, how='left').fillna(0)\n\n# # Don't use old data from year 2013\n#all_data = all_data[all_data['date_block_num'] >= 12] \n\n# # List of all lagged features\n# fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# # We will drop these at fitting stage\n# to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# # Category for each item\n# item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\ntest_data = pd.merge(test_data, item_category_mapping, how='left', on='item_id')\ntest_data = downcast_dtypes(test_data)\ngc.collect();\n\n# print(test_data.head)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.clip(0,20)\n\nlgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nmodel = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train = y_train.clip(0,20)\n# print(np.max(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop_cols = ['ID','date_block_num']\nX_test =  test_data.drop(to_drop_cols, axis=1)\n# print(X_train.columns)\n# print(X_test.columns)\npred_lgb = model.predict(X_test)\n#print(pred_lgb)\n\nmerged = test.copy()\nmerged['item_cnt_month'] = pred_lgb\nmerged['item_cnt_month']=merged['item_cnt_month'].clip(lower=0,upper=20)\nmerged=merged.drop(['shop_id','item_id'],axis=1)\nmerged['ID']=merged['ID'].astype('int')\nmerged.to_csv(\"lightgbm_lag_trainclipped.csv\",index=False)\nprint(merged.head)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}